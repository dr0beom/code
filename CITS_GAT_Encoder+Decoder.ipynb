{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b823def",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from scipy import sparse as sp\n",
    "import random\n",
    "import numpy as np\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32bab4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder\n",
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features, n_heads, d_h):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.hidden_features = hidden_features\n",
    "        self.d_h = d_h\n",
    "\n",
    "        self.linear1 = nn.Linear(in_features, hidden_features)\n",
    "        self.linear2 = nn.Linear(hidden_features, out_features)\n",
    "        self.attn_linear1 = nn.Linear(hidden_features, d_h)\n",
    "        self.attn_linear2 = nn.Linear(hidden_features, d_h)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, x, v_prev, neighbors):\n",
    "        n_nodes = x.shape[0]\n",
    "\n",
    "        v_prev = self.linear1(v_prev)\n",
    "        v_prev = v_prev.unsqueeze(0).repeat(n_nodes, 1)\n",
    "\n",
    "        neighbors = self.linear1(neighbors)\n",
    "\n",
    "        attn_input = torch.cat([v_prev, neighbors], dim=-1)\n",
    "        attn_input = self.activation(attn_input)\n",
    "\n",
    "        attn1 = self.attn_linear1(attn_input)\n",
    "        attn2 = self.attn_linear2(attn_input)\n",
    "\n",
    "        attn_output = torch.matmul(attn1, attn2.transpose(0, 1)) / (self.d_h ** 0.5)\n",
    "        attn_output = self.activation(attn_output)\n",
    "\n",
    "        masked_attn_output = attn_output.masked_fill(neighbors == 0, float('-inf'))\n",
    "        attn_weights = self.softmax(masked_attn_output)\n",
    "\n",
    "        x = self.linear2(x)\n",
    "        x = x.unsqueeze(0).repeat(n_nodes, 1, 1)\n",
    "\n",
    "        output = torch.matmul(attn_weights.unsqueeze(1), x)\n",
    "        output = output.squeeze(1)\n",
    "\n",
    "        return output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebb01fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "class GraphAttentionLayer(torch.nn.Module):\n",
    "    def __init__(self, in_features, out_features, n_heads, is_concat = True, dropout = 0.6, leacky_relu_negative_slope = 0.2):\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        self.W = torch.nn.Parameter(torch.randn(in_features, out_features))\n",
    "        self.is_concat = is_concat\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        if is_concat:\n",
    "            assert out_features % n_heads == 0\n",
    "\n",
    "            self.n_hidden = out_features // n_heads\n",
    "        else:\n",
    "            self.n_hidden = out_features\n",
    "\n",
    "        self.linear = nn.Linear(in_features, self.n_hidden * n_heads, bias = False)\n",
    "\n",
    "        self.attn = nn.Linear(self.n_hidden * 2, 1, bias = False)\n",
    "        self.activation = nn.LeakyReLU(negative_slope = leacky_relu_negative_slope)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # self.decoder = Decoder(self.n_hidden)\n",
    "        \n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        n_nodes = x.shape[0]\n",
    "        g=self.linear(x).view(n_nodes, self.n_heads, self.n_hidden)\n",
    "        g_repeat = g.repeat(n_nodes, 1,1)\n",
    "        g_repeat_interleave = g.repeat_interleave(n_nodes, dim=0)\n",
    "        g_concat = torch.cat([g_repeat_interleave, g_repeat], dim = -1)\n",
    "        g_concat = g_concat.view(n_nodes, n_nodes, self.n_heads, 2 * self.n_hidden)\n",
    "        e = self.activation(self.attn(g_concat))\n",
    "        e = e.squeeze(-1)\n",
    "        assert adj.shape[0] == 1 or adj.shape[0] == n_nodes\n",
    "        assert adj.shape[1] == 1 or adj.shape[1] == n_nodes\n",
    "        assert adj.shape[2] == 1 or adj.shape[2] == self.n_heads\n",
    "        e=e.masked_fill(adj == 0, 1)\n",
    "        a = self.softmax(e)\n",
    "        a = self.dropout(a)\n",
    "        attn_res = torch.einsum('ijh,jhf->ihf', a, g)\n",
    "        if self.is_concat:\n",
    "            return attn_res.reshape(n_nodes, self.n_heads * self.n_hidden)\n",
    "        else:\n",
    "            return attn_res.mean(dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78d34db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features, n_heads, d_h):\n",
    "        super(GAT, self).__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.attention1 = GraphAttentionLayer(in_features, hidden_features, n_heads)\n",
    "        self.attention2 = GraphAttentionLayer(hidden_features, out_features, n_heads)\n",
    "        self.norm= nn.LayerNorm(out_features)\n",
    "        self.decoder = Decoder(out_features, hidden_features, out_features, n_heads, d_h)\n",
    "    \n",
    "    def forward(self, x, adj):\n",
    "        x = self.attention1(x, adj)\n",
    "        x = self.attention2(x, adj)\n",
    "        x = self.norm(x)\n",
    "        x = F.softmax(x, dim=-1)\n",
    "        return x\n",
    "    \n",
    "    def decode(self, x, v_prev, neighbors):\n",
    "        return self.decoder(x, v_prev, neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34e65994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph 1 - Feature Matrix:\n",
      "tensor([[ 0.3828, -0.8849, -1.5154,  0.2180,  1.4070,  0.0797, -0.6923, -0.0117],\n",
      "        [-0.7997,  0.6889,  0.1618, -0.0541,  0.3779,  2.0618,  2.1502,  2.1678],\n",
      "        [-0.3418, -0.0420,  0.0476, -0.6429,  1.7629,  0.3875,  0.2837, -0.9960],\n",
      "        [-2.8452,  0.2046, -0.6345, -0.9439,  0.8092, -0.7611, -1.6979, -0.2157]])\n",
      "\n",
      "Graph 1 - Adjacency Tensor:\n",
      "tensor([[[1., 1., 0., 0.],\n",
      "         [1., 1., 0., 0.],\n",
      "         [1., 1., 0., 0.],\n",
      "         [1., 1., 0., 0.]],\n",
      "\n",
      "        [[1., 1., 1., 0.],\n",
      "         [1., 1., 1., 0.],\n",
      "         [1., 1., 1., 0.],\n",
      "         [1., 1., 1., 0.]],\n",
      "\n",
      "        [[0., 1., 1., 1.],\n",
      "         [0., 1., 1., 1.],\n",
      "         [0., 1., 1., 1.],\n",
      "         [0., 1., 1., 1.]],\n",
      "\n",
      "        [[0., 0., 1., 1.],\n",
      "         [0., 0., 1., 1.],\n",
      "         [0., 0., 1., 1.],\n",
      "         [0., 0., 1., 1.]]])\n",
      "\n",
      "\n",
      "Graph 2 - Feature Matrix:\n",
      "tensor([[ 0.9009,  1.3300, -0.6778,  0.7949,  0.8382,  1.1925, -2.2364,  1.8151],\n",
      "        [-0.7353,  0.2957, -1.0917, -1.2875, -0.5308,  0.2467,  0.6887, -0.3103],\n",
      "        [ 0.0160,  1.2762, -0.3417,  1.0431, -1.1058,  0.2464,  0.9397,  0.4885],\n",
      "        [ 0.1493, -0.3956,  0.5331,  1.2716, -0.9781, -0.5786,  0.1953, -0.3893],\n",
      "        [ 0.4306, -1.3344,  0.3070, -1.7779, -1.5201,  0.8438, -0.2512,  0.1456]])\n",
      "\n",
      "Graph 2 - Adjacency Tensor:\n",
      "tensor([[[1., 1., 0., 0., 0.],\n",
      "         [1., 1., 0., 0., 0.],\n",
      "         [1., 1., 0., 0., 0.],\n",
      "         [1., 1., 0., 0., 0.],\n",
      "         [1., 1., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 1., 1., 0., 0.],\n",
      "         [1., 1., 1., 0., 0.],\n",
      "         [1., 1., 1., 0., 0.],\n",
      "         [1., 1., 1., 0., 0.],\n",
      "         [1., 1., 1., 0., 0.]],\n",
      "\n",
      "        [[0., 1., 1., 1., 0.],\n",
      "         [0., 1., 1., 1., 0.],\n",
      "         [0., 1., 1., 1., 0.],\n",
      "         [0., 1., 1., 1., 0.],\n",
      "         [0., 1., 1., 1., 0.]],\n",
      "\n",
      "        [[0., 0., 1., 1., 1.],\n",
      "         [0., 0., 1., 1., 1.],\n",
      "         [0., 0., 1., 1., 1.],\n",
      "         [0., 0., 1., 1., 1.],\n",
      "         [0., 0., 1., 1., 1.]],\n",
      "\n",
      "        [[0., 0., 0., 1., 1.],\n",
      "         [0., 0., 0., 1., 1.],\n",
      "         [0., 0., 0., 1., 1.],\n",
      "         [0., 0., 0., 1., 1.],\n",
      "         [0., 0., 0., 1., 1.]]])\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\park\\AppData\\Local\\Temp\\ipykernel_6252\\1035874526.py:8: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  adj_matrix1 = nx.adjacency_matrix(G1)\n",
      "C:\\Users\\park\\AppData\\Local\\Temp\\ipykernel_6252\\1035874526.py:32: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  adj_matrix2 = nx.adjacency_matrix(G2)\n",
      "C:\\Users\\park\\AppData\\Local\\Temp\\ipykernel_6252\\1035874526.py:50: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  graph_array= np.array(graph_list, dtype=object)\n"
     ]
    }
   ],
   "source": [
    "# Create multiple dummy graphs with different node sizes\n",
    "graph_list =[]\n",
    "# Graph 1\n",
    "G1 = nx.Graph()\n",
    "G1.add_nodes_from(range(4))  # Add nodes\n",
    "G1.add_edges_from([(0, 1), (1, 2), (2, 3)])  # Add edges\n",
    "\n",
    "adj_matrix1 = nx.adjacency_matrix(G1)\n",
    "adj_matrix1 = adj_matrix1 + sp.eye(adj_matrix1.shape[0])  # Add self-loop\n",
    "adj_tensor1 = torch.Tensor(adj_matrix1.todense())\n",
    "\n",
    "\n",
    "num_nodes1 = G1.number_of_nodes()\n",
    "in_features1 = 8\n",
    "x1 = torch.randn(num_nodes1, in_features1)\n",
    "\n",
    "# Resize adjacency tensor to match the input features size\n",
    "adj_tensor1 = adj_tensor1.unsqueeze(0)  # Add an extra dimension\n",
    "adj_tensor1 = adj_tensor1.repeat(num_nodes1, 1, 1)  # Repeat the adjacency tensor\n",
    "adj_tensor1 = adj_tensor1.transpose(0, 1)  # Transpose the dimensions\n",
    "\n",
    "# Generate labels for Graph 1\n",
    "labels1 = torch.randint(0, 2, (num_nodes1,)).to(device)\n",
    "\n",
    "graph_list.append((x1, adj_tensor1))\n",
    "\n",
    "# Graph 2\n",
    "G2 = nx.Graph()\n",
    "G2.add_nodes_from(range(5))  # Add nodes\n",
    "G2.add_edges_from([(0, 1), (1, 2), (2, 3), (3, 4)])  # Add edges\n",
    "\n",
    "adj_matrix2 = nx.adjacency_matrix(G2)\n",
    "adj_matrix2 = adj_matrix2 + sp.eye(adj_matrix2.shape[0])  # Add self-loop\n",
    "adj_tensor2 = torch.Tensor(adj_matrix2.todense())\n",
    "\n",
    "num_nodes2 = G2.number_of_nodes()\n",
    "in_features2 = 8\n",
    "x2 = torch.randn(num_nodes2, in_features2)\n",
    "\n",
    "# Resize adjacency tensor to match the input features size\n",
    "adj_tensor2 = adj_tensor2.unsqueeze(0)  # Add an extra dimension\n",
    "adj_tensor2 = adj_tensor2.repeat(num_nodes2, 1, 1)  # Repeat the adjacency tensor\n",
    "adj_tensor2 = adj_tensor2.transpose(0, 1)  # Transpose the dimensions\n",
    "\n",
    "# Generate labels for Graph 2\n",
    "labels2 = torch.randint(0, 2, (num_nodes2,)).to(device)\n",
    "\n",
    "graph_list.append((x2, adj_tensor2))\n",
    "\n",
    "graph_array= np.array(graph_list, dtype=object)\n",
    "\n",
    "# Access the graphs and their components from the graph list\n",
    "for i, (feature_matrix, adj_tensor) in enumerate(graph_array):\n",
    "    # # Expand the adj_tensor dimensions if using multiple attention heads\n",
    "    # if adj_tensor.dim() == 2:\n",
    "    #     adj_tensor = adj_tensor.unsqueeze(2).expand(-1, -1, self.n_heads)\n",
    "\n",
    "    graph_list[i] = (feature_matrix.cuda(), adj_tensor.cuda())\n",
    "\n",
    "\n",
    "    print(f\"Graph {i+1} - Feature Matrix:\")\n",
    "    print(feature_matrix)\n",
    "\n",
    "    print(f\"\\nGraph {i+1} - Adjacency Tensor:\")\n",
    "    print(adj_tensor)\n",
    "\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24b45e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph 1 - Output:\n",
      "tensor([[0.1569, 0.0680, 0.0590, 0.0096, 0.0605, 0.1869, 0.2752, 0.1839],\n",
      "        [0.0793, 0.0152, 0.0605, 0.0605, 0.0263, 0.5897, 0.0725, 0.0962],\n",
      "        [0.0708, 0.0684, 0.0519, 0.0123, 0.0415, 0.4191, 0.1866, 0.1494],\n",
      "        [0.0796, 0.0668, 0.0694, 0.0094, 0.0761, 0.1948, 0.3323, 0.1716]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 2 - Output:\n",
      "tensor([[0.0929, 0.0220, 0.0740, 0.0372, 0.0179, 0.0209, 0.0300, 0.0300, 0.0816,\n",
      "         0.5934],\n",
      "        [0.4777, 0.0128, 0.2055, 0.0226, 0.0478, 0.0478, 0.0329, 0.0554, 0.0311,\n",
      "         0.0663],\n",
      "        [0.3601, 0.0081, 0.1657, 0.0322, 0.0386, 0.0559, 0.0620, 0.1599, 0.0406,\n",
      "         0.0769],\n",
      "        [0.2098, 0.0072, 0.1662, 0.0272, 0.0728, 0.0507, 0.0417, 0.1662, 0.1689,\n",
      "         0.0894],\n",
      "        [0.3733, 0.0056, 0.0968, 0.0610, 0.0528, 0.0528, 0.0769, 0.0533, 0.1088,\n",
      "         0.1187]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Create and initialize the GAT models for each graph\n",
    "gat_models = []\n",
    "for i, (feature_matrix, adj_tensor) in enumerate(graph_array):\n",
    "    in_features = feature_matrix.shape[1]\n",
    "    n_heads = adj_tensor.shape[2]\n",
    "    hidden_features = 4 * n_heads\n",
    "    out_features = 2 * n_heads\n",
    "    d_h = 4 * n_heads\n",
    "    gat_model = GAT(in_features, hidden_features, out_features, n_heads, d_h).cuda()\n",
    "    gat_models.append(gat_model)\n",
    "    feature_matrix = feature_matrix.cuda()\n",
    "    adj_tensor = adj_tensor.cuda()\n",
    "    output = gat_model(feature_matrix, adj_tensor)\n",
    "    print(f\"Graph {i+1} - Output:\")\n",
    "    print(output)\n",
    "    #output : 각 노드에 대한 클래스 라벨 예측 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26453945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph 1: Epoch: 001, Loss: -0.0709\n",
      "Graph 2: Epoch: 001, Loss: -0.0709\n",
      "Graph 1: Epoch: 002, Loss: -0.1487\n",
      "Graph 2: Epoch: 002, Loss: -0.1487\n",
      "Graph 1: Epoch: 003, Loss: -0.1270\n",
      "Graph 2: Epoch: 003, Loss: -0.1270\n",
      "Graph 1: Epoch: 004, Loss: -0.2161\n",
      "Graph 2: Epoch: 004, Loss: -0.2161\n",
      "Graph 1: Epoch: 005, Loss: -0.1919\n",
      "Graph 2: Epoch: 005, Loss: -0.1919\n",
      "Graph 1: Epoch: 006, Loss: -0.1337\n",
      "Graph 2: Epoch: 006, Loss: -0.1337\n",
      "Graph 1: Epoch: 007, Loss: -0.2494\n",
      "Graph 2: Epoch: 007, Loss: -0.2494\n",
      "Graph 1: Epoch: 008, Loss: -0.2540\n",
      "Graph 2: Epoch: 008, Loss: -0.2540\n",
      "Graph 1: Epoch: 009, Loss: -0.2088\n",
      "Graph 2: Epoch: 009, Loss: -0.2088\n",
      "Graph 1: Epoch: 010, Loss: -0.1941\n",
      "Graph 2: Epoch: 010, Loss: -0.1941\n",
      "Graph 1: Epoch: 011, Loss: -0.3176\n",
      "Graph 2: Epoch: 011, Loss: -0.3176\n",
      "Graph 1: Epoch: 012, Loss: -0.2908\n",
      "Graph 2: Epoch: 012, Loss: -0.2908\n",
      "Graph 1: Epoch: 013, Loss: -0.4043\n",
      "Graph 2: Epoch: 013, Loss: -0.4043\n",
      "Graph 1: Epoch: 014, Loss: -0.1683\n",
      "Graph 2: Epoch: 014, Loss: -0.1683\n",
      "Graph 1: Epoch: 015, Loss: -0.2577\n",
      "Graph 2: Epoch: 015, Loss: -0.2577\n",
      "Graph 1: Epoch: 016, Loss: -0.2935\n",
      "Graph 2: Epoch: 016, Loss: -0.2935\n",
      "Graph 1: Epoch: 017, Loss: -0.4561\n",
      "Graph 2: Epoch: 017, Loss: -0.4561\n",
      "Graph 1: Epoch: 018, Loss: -0.0676\n",
      "Graph 2: Epoch: 018, Loss: -0.0676\n",
      "Graph 1: Epoch: 019, Loss: -0.1866\n",
      "Graph 2: Epoch: 019, Loss: -0.1866\n",
      "Graph 1: Epoch: 020, Loss: -0.3510\n",
      "Graph 2: Epoch: 020, Loss: -0.3510\n",
      "Graph 1: Epoch: 021, Loss: -0.2753\n",
      "Graph 2: Epoch: 021, Loss: -0.2753\n",
      "Graph 1: Epoch: 022, Loss: -0.2321\n",
      "Graph 2: Epoch: 022, Loss: -0.2321\n",
      "Graph 1: Epoch: 023, Loss: -0.2572\n",
      "Graph 2: Epoch: 023, Loss: -0.2572\n",
      "Graph 1: Epoch: 024, Loss: -0.2543\n",
      "Graph 2: Epoch: 024, Loss: -0.2543\n",
      "Graph 1: Epoch: 025, Loss: -0.2151\n",
      "Graph 2: Epoch: 025, Loss: -0.2151\n",
      "Graph 1: Epoch: 026, Loss: -0.2299\n",
      "Graph 2: Epoch: 026, Loss: -0.2299\n",
      "Graph 1: Epoch: 027, Loss: -0.3463\n",
      "Graph 2: Epoch: 027, Loss: -0.3463\n",
      "Graph 1: Epoch: 028, Loss: -0.0394\n",
      "Graph 2: Epoch: 028, Loss: -0.0394\n",
      "Graph 1: Epoch: 029, Loss: -0.3366\n",
      "Graph 2: Epoch: 029, Loss: -0.3366\n",
      "Graph 1: Epoch: 030, Loss: -0.2263\n",
      "Graph 2: Epoch: 030, Loss: -0.2263\n",
      "Graph 1: Epoch: 031, Loss: -0.3267\n",
      "Graph 2: Epoch: 031, Loss: -0.3267\n",
      "Graph 1: Epoch: 032, Loss: -0.2277\n",
      "Graph 2: Epoch: 032, Loss: -0.2277\n",
      "Graph 1: Epoch: 033, Loss: -0.1507\n",
      "Graph 2: Epoch: 033, Loss: -0.1507\n",
      "Graph 1: Epoch: 034, Loss: -0.3778\n",
      "Graph 2: Epoch: 034, Loss: -0.3778\n",
      "Graph 1: Epoch: 035, Loss: -0.2744\n",
      "Graph 2: Epoch: 035, Loss: -0.2744\n",
      "Graph 1: Epoch: 036, Loss: -0.2798\n",
      "Graph 2: Epoch: 036, Loss: -0.2798\n",
      "Graph 1: Epoch: 037, Loss: -0.3028\n",
      "Graph 2: Epoch: 037, Loss: -0.3028\n",
      "Graph 1: Epoch: 038, Loss: -0.3740\n",
      "Graph 2: Epoch: 038, Loss: -0.3740\n",
      "Graph 1: Epoch: 039, Loss: -0.5704\n",
      "Graph 2: Epoch: 039, Loss: -0.5704\n",
      "Graph 1: Epoch: 040, Loss: -0.1854\n",
      "Graph 2: Epoch: 040, Loss: -0.1854\n",
      "Graph 1: Epoch: 041, Loss: -0.3616\n",
      "Graph 2: Epoch: 041, Loss: -0.3616\n",
      "Graph 1: Epoch: 042, Loss: -0.3450\n",
      "Graph 2: Epoch: 042, Loss: -0.3450\n",
      "Graph 1: Epoch: 043, Loss: -0.2541\n",
      "Graph 2: Epoch: 043, Loss: -0.2541\n",
      "Graph 1: Epoch: 044, Loss: -0.3320\n",
      "Graph 2: Epoch: 044, Loss: -0.3320\n",
      "Graph 1: Epoch: 045, Loss: -0.3539\n",
      "Graph 2: Epoch: 045, Loss: -0.3539\n",
      "Graph 1: Epoch: 046, Loss: -0.1622\n",
      "Graph 2: Epoch: 046, Loss: -0.1622\n",
      "Graph 1: Epoch: 047, Loss: -0.3335\n",
      "Graph 2: Epoch: 047, Loss: -0.3335\n",
      "Graph 1: Epoch: 048, Loss: -0.3744\n",
      "Graph 2: Epoch: 048, Loss: -0.3744\n",
      "Graph 1: Epoch: 049, Loss: -0.3326\n",
      "Graph 2: Epoch: 049, Loss: -0.3326\n",
      "Graph 1: Epoch: 050, Loss: -0.1872\n",
      "Graph 2: Epoch: 050, Loss: -0.1872\n",
      "Graph 1: Epoch: 051, Loss: -0.4077\n",
      "Graph 2: Epoch: 051, Loss: -0.4077\n",
      "Graph 1: Epoch: 052, Loss: -0.3684\n",
      "Graph 2: Epoch: 052, Loss: -0.3684\n",
      "Graph 1: Epoch: 053, Loss: -0.4214\n",
      "Graph 2: Epoch: 053, Loss: -0.4214\n",
      "Graph 1: Epoch: 054, Loss: -0.4785\n",
      "Graph 2: Epoch: 054, Loss: -0.4785\n",
      "Graph 1: Epoch: 055, Loss: -0.2885\n",
      "Graph 2: Epoch: 055, Loss: -0.2885\n",
      "Graph 1: Epoch: 056, Loss: -0.2795\n",
      "Graph 2: Epoch: 056, Loss: -0.2795\n",
      "Graph 1: Epoch: 057, Loss: -0.2174\n",
      "Graph 2: Epoch: 057, Loss: -0.2174\n",
      "Graph 1: Epoch: 058, Loss: -0.2631\n",
      "Graph 2: Epoch: 058, Loss: -0.2631\n",
      "Graph 1: Epoch: 059, Loss: -0.2885\n",
      "Graph 2: Epoch: 059, Loss: -0.2885\n",
      "Graph 1: Epoch: 060, Loss: -0.3038\n",
      "Graph 2: Epoch: 060, Loss: -0.3038\n",
      "Graph 1: Epoch: 061, Loss: -0.2979\n",
      "Graph 2: Epoch: 061, Loss: -0.2979\n",
      "Graph 1: Epoch: 062, Loss: -0.4347\n",
      "Graph 2: Epoch: 062, Loss: -0.4347\n",
      "Graph 1: Epoch: 063, Loss: -0.3246\n",
      "Graph 2: Epoch: 063, Loss: -0.3246\n",
      "Graph 1: Epoch: 064, Loss: -0.1941\n",
      "Graph 2: Epoch: 064, Loss: -0.1941\n",
      "Graph 1: Epoch: 065, Loss: -0.2850\n",
      "Graph 2: Epoch: 065, Loss: -0.2850\n",
      "Graph 1: Epoch: 066, Loss: -0.1519\n",
      "Graph 2: Epoch: 066, Loss: -0.1519\n",
      "Graph 1: Epoch: 067, Loss: -0.2659\n",
      "Graph 2: Epoch: 067, Loss: -0.2659\n",
      "Graph 1: Epoch: 068, Loss: -0.3054\n",
      "Graph 2: Epoch: 068, Loss: -0.3054\n",
      "Graph 1: Epoch: 069, Loss: -0.3953\n",
      "Graph 2: Epoch: 069, Loss: -0.3953\n",
      "Graph 1: Epoch: 070, Loss: -0.4546\n",
      "Graph 2: Epoch: 070, Loss: -0.4546\n",
      "Graph 1: Epoch: 071, Loss: -0.4262\n",
      "Graph 2: Epoch: 071, Loss: -0.4262\n",
      "Graph 1: Epoch: 072, Loss: -0.3316\n",
      "Graph 2: Epoch: 072, Loss: -0.3316\n",
      "Graph 1: Epoch: 073, Loss: -0.2516\n",
      "Graph 2: Epoch: 073, Loss: -0.2516\n",
      "Graph 1: Epoch: 074, Loss: -0.3057\n",
      "Graph 2: Epoch: 074, Loss: -0.3057\n",
      "Graph 1: Epoch: 075, Loss: -0.3343\n",
      "Graph 2: Epoch: 075, Loss: -0.3343\n",
      "Graph 1: Epoch: 076, Loss: -0.2645\n",
      "Graph 2: Epoch: 076, Loss: -0.2645\n",
      "Graph 1: Epoch: 077, Loss: -0.2881\n",
      "Graph 2: Epoch: 077, Loss: -0.2881\n",
      "Graph 1: Epoch: 078, Loss: -0.2541\n",
      "Graph 2: Epoch: 078, Loss: -0.2541\n",
      "Graph 1: Epoch: 079, Loss: -0.1406\n",
      "Graph 2: Epoch: 079, Loss: -0.1406\n",
      "Graph 1: Epoch: 080, Loss: -0.6942\n",
      "Graph 2: Epoch: 080, Loss: -0.6942\n",
      "Graph 1: Epoch: 081, Loss: -0.4096\n",
      "Graph 2: Epoch: 081, Loss: -0.4096\n",
      "Graph 1: Epoch: 082, Loss: -0.3384\n",
      "Graph 2: Epoch: 082, Loss: -0.3384\n",
      "Graph 1: Epoch: 083, Loss: -0.2867\n",
      "Graph 2: Epoch: 083, Loss: -0.2867\n",
      "Graph 1: Epoch: 084, Loss: -0.3674\n",
      "Graph 2: Epoch: 084, Loss: -0.3674\n",
      "Graph 1: Epoch: 085, Loss: -0.5280\n",
      "Graph 2: Epoch: 085, Loss: -0.5280\n",
      "Graph 1: Epoch: 086, Loss: -0.2783\n",
      "Graph 2: Epoch: 086, Loss: -0.2783\n",
      "Graph 1: Epoch: 087, Loss: -0.4988\n",
      "Graph 2: Epoch: 087, Loss: -0.4988\n",
      "Graph 1: Epoch: 088, Loss: -0.3222\n",
      "Graph 2: Epoch: 088, Loss: -0.3222\n",
      "Graph 1: Epoch: 089, Loss: -0.2425\n",
      "Graph 2: Epoch: 089, Loss: -0.2425\n",
      "Graph 1: Epoch: 090, Loss: -0.4322\n",
      "Graph 2: Epoch: 090, Loss: -0.4322\n",
      "Graph 1: Epoch: 091, Loss: -0.3045\n",
      "Graph 2: Epoch: 091, Loss: -0.3045\n",
      "Graph 1: Epoch: 092, Loss: -0.2221\n",
      "Graph 2: Epoch: 092, Loss: -0.2221\n",
      "Graph 1: Epoch: 093, Loss: -0.5246\n",
      "Graph 2: Epoch: 093, Loss: -0.5246\n",
      "Graph 1: Epoch: 094, Loss: -0.1481\n",
      "Graph 2: Epoch: 094, Loss: -0.1481\n",
      "Graph 1: Epoch: 095, Loss: -0.3028\n",
      "Graph 2: Epoch: 095, Loss: -0.3028\n",
      "Graph 1: Epoch: 096, Loss: -0.3555\n",
      "Graph 2: Epoch: 096, Loss: -0.3555\n",
      "Graph 1: Epoch: 097, Loss: -0.4464\n",
      "Graph 2: Epoch: 097, Loss: -0.4464\n",
      "Graph 1: Epoch: 098, Loss: -0.4040\n",
      "Graph 2: Epoch: 098, Loss: -0.4040\n",
      "Graph 1: Epoch: 099, Loss: -0.3320\n",
      "Graph 2: Epoch: 099, Loss: -0.3320\n",
      "Graph 1: Epoch: 100, Loss: -0.2600\n",
      "Graph 2: Epoch: 100, Loss: -0.2600\n"
     ]
    }
   ],
   "source": [
    "# Set the optimizer and loss function\n",
    "optimizer = optim.Adam(gat_model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.NLLLoss()\n",
    "\n",
    "# Move the model and loss function to the GPU\n",
    "gat_model = gat_model.cuda()\n",
    "criterion = criterion.cuda()\n",
    "\n",
    "# Training loop\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "    for graph_idx, (feature_matrix, adj_tensor) in enumerate(graph_array):\n",
    "        feature_matrix = feature_matrix.to(device)\n",
    "        adj_tensor = adj_tensor.to(device)\n",
    "         # Generate random labels for the current graph\n",
    "        num_nodes = feature_matrix.shape[0]\n",
    "        labels = torch.tensor([random.randint(0, 1) for _ in range(num_nodes)]).to(device)\n",
    "                \n",
    "        # Zero the gradients\n",
    "        gat_model.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = gat_models[graph_idx](feature_matrix, adj_tensor)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(output.squeeze(0), labels)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Calculate the average loss for the epoch\n",
    "    average_loss = total_loss / len(graph_array)\n",
    "    \n",
    "    for graph_idx in range(len(graph_list)):\n",
    "        print(\"Graph {}: Epoch: {:03d}, Loss: {:.4f}\".format(graph_idx+1, epoch+1, average_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e48d6d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python39",
   "language": "python",
   "name": "python39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
